# Application Configuration
DB_DSN="postgres://postgres:postgres@localhost:5432/thyris?sslmode=disable&TimeZone=Europe/Istanbul"
REDIS_URL="redis://:thyrisredis@localhost:6379/0"
SERVER_PORT="8080"

# Admin & Security
ADMIN_API_KEY="change-me-admin-key"

# Confidence Thresholds (can be overridden per pattern / category)
CONFIDENCE_ALLOW_THRESHOLD=0.30
CONFIDENCE_BLOCK_THRESHOLD=0.85
CONFIDENCE_PII_THRESHOLD=0.60
CONFIDENCE_SECRET_THRESHOLD=0.65
CONFIDENCE_INJECTION_THRESHOLD=0.70

# SIEM / Webhook (optional)
# If empty, SIEM export is disabled
SIEM_WEBHOOK_URL=""

# Feature Flags (Enable/Disable capabilities)
FEATURE_AI_SEMANTIC_ANALYSIS=true
FEATURE_JSON_SCHEMA_VALIDATION=true

# AI Configuration
# Provider selection: OPENAI_COMPATIBLE (default) or BEDROCK
AI_PROVIDER="OPENAI_COMPATIBLE"

# OpenAI-Compatible Provider Settings
# For local Ollama, use http://localhost:11434/v1
# If running in Docker, use http://host.docker.internal:11434/v1
# For OpenAI, use https://api.openai.com/v1
AI_MODEL_URL="http://localhost:11434/v1"
AI_API_KEY="ollama"
AI_MODEL="llama3.1:8b"

# AWS Bedrock Provider Settings (only used when AI_PROVIDER="BEDROCK")
# Region is required when using Bedrock (e.g., us-east-1, eu-central-1, eu-west-1)
AWS_BEDROCK_REGION=""
# Optional: Custom endpoint URL for VPC endpoints or testing
AWS_BEDROCK_ENDPOINT_OVERRIDE=""
# Model ID for Bedrock (e.g., anthropic.claude-3-sonnet-20240229-v1:0, amazon.titan-text-express-v1)
# Supported model families: Anthropic Claude, Amazon Titan, Meta Llama, Mistral, Cohere
AWS_BEDROCK_MODEL_ID="anthropic.claude-3-sonnet-20240229-v1:0"

# AWS Credentials (required for Bedrock)
# Option 1: Use environment variables (recommended for development/testing)
# Get these from AWS Console → IAM → Users → Security Credentials → Create Access Key
AWS_ACCESS_KEY_ID=""
AWS_SECRET_ACCESS_KEY=""
# Optional: For temporary credentials (STS, assumed roles)
AWS_SESSION_TOKEN=""

# Option 2: Use shared credentials file (~/.aws/credentials) - leave above empty
# Option 3: Use IAM role (EC2, ECS, Lambda) - leave above empty, role is auto-detected

# Note: AWS credentials are loaded via standard AWS credential chain in this order:
# 1. Environment variables (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_SESSION_TOKEN)
# 2. Shared credentials file (~/.aws/credentials)
# 3. IAM role (when running on EC2, ECS, Lambda, etc.)

# Streaming / Gateway Settings
# Maximum size of the in-memory buffer used for streaming output guardrails (in bytes).
# Default: 262144 (~256 KB). Set to 0 or negative to disable the limit (not recommended in production).
STREAM_MAX_BUFFER_BYTES=262144
# Behaviour when streaming events cannot be parsed or other non-guardrail errors occur.
# Supported values: LENIENT (default, forward raw event) or STRICT (stop stream and return error).
STREAM_FAIL_MODE="LENIENT"

# Gateway block behaviour for guardrails/validators (TOXIC_LANGUAGE, PROMPT_INJECTION, etc.)
# Controls HTTP-level blocking behaviour; core PII/blacklist decisions are still driven by PII_MODE
# Supported values:
# - BLOCK (default): If any guardrail/validator returns "blocked=true", respond with 4xx + OpenAI-style error
# - MASK: Always HTTP 200; the LLM response is returned and blocked content is masked, with status exposed via tsz_meta.input/output[*].blocked
# - WARN: Same as MASK at HTTP level; intended to be interpreted as a soft warning by the client
GATEWAY_BLOCK_MODE="BLOCK"
# PII handling behaviour for core detection engine endpoints (/detect, /patterns, /allowlist, /blacklist)
# Controls how detected PII is returned in responses and influences core decision logic.
# Supported values:
# - MASK (default): Detected PII is replaced with mask tokens in the response payload while still allowing the request.
# - BLOCK: Requests containing PII are blocked at the core engine level; no raw PII is returned in the response body.
# Note: This setting affects only core detection/validation endpoints; HTTP-level blocking for gateway streaming is controlled by GATEWAY_BLOCK_MODE.
PII_MODE="MASK"
